{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYWgPy/oP8XHVQZW7Whqbo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vramonlinebsc/neural_operator_surrogates/blob/main/neural_operator_surrogates_checkpointed_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5ZMs1Fy9u8A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.sparse as sp\n",
        "from scipy.sparse.linalg import expm_multiply\n",
        "from scipy.linalg import expm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "from itertools import product\n",
        "from typing import Tuple, List, Dict, Optional\n",
        "import json\n",
        "import pickle\n",
        "import os\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, asdict\n",
        "import hashlib\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# ============================================================================\n",
        "# CHECKPOINT MANAGEMENT\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class ExperimentConfig:\n",
        "    \"\"\"Configuration for experiments\"\"\"\n",
        "    N_values: List[int]\n",
        "    topologies: List[str]\n",
        "    n_train_samples: int\n",
        "    n_val_samples: int\n",
        "    T: int\n",
        "    dt: float\n",
        "    epochs: int\n",
        "    batch_size: int\n",
        "    lr: float\n",
        "    modes: int\n",
        "    width: int\n",
        "    n_layers: int\n",
        "\n",
        "    def get_hash(self) -> str:\n",
        "        \"\"\"Get unique hash for this configuration\"\"\"\n",
        "        config_str = json.dumps(asdict(self), sort_keys=True)\n",
        "        return hashlib.md5(config_str.encode()).hexdigest()[:8]\n",
        "\n",
        "\n",
        "class CheckpointManager:\n",
        "    \"\"\"Manages checkpoints for resumable experiments\"\"\"\n",
        "\n",
        "    def __init__(self, base_dir: str = \"checkpoints\"):\n",
        "        self.base_dir = Path(base_dir)\n",
        "        self.base_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    def save_dataset(self, dataset, N: int, topology: str, split: str):\n",
        "        \"\"\"Save dataset to disk\"\"\"\n",
        "        path = self.base_dir / f\"dataset_N{N}_{topology}_{split}.pkl\"\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump(dataset.data, f)\n",
        "        print(f\"  ✓ Saved dataset: {path}\")\n",
        "\n",
        "    def load_dataset(self, N: int, topology: str, split: str, T: int, dt: float):\n",
        "        \"\"\"Load dataset from disk\"\"\"\n",
        "        path = self.base_dir / f\"dataset_N{N}_{topology}_{split}.pkl\"\n",
        "        if path.exists():\n",
        "            print(f\"  ✓ Loading existing dataset: {path}\")\n",
        "            dataset = NMRDataset(N, topology, 0, T, dt)  # Empty dataset\n",
        "            with open(path, 'rb') as f:\n",
        "                dataset.data = pickle.load(f)\n",
        "            return dataset\n",
        "        return None\n",
        "\n",
        "    def save_model(self, model: nn.Module, N: int, topology: str, epoch: int):\n",
        "        \"\"\"Save model checkpoint\"\"\"\n",
        "        path = self.base_dir / f\"model_N{N}_{topology}_epoch{epoch}.pt\"\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'N': N,\n",
        "            'topology': topology\n",
        "        }, path)\n",
        "        print(f\"  ✓ Saved model checkpoint: {path}\")\n",
        "\n",
        "    def load_model(self, model: nn.Module, N: int, topology: str) -> Optional[int]:\n",
        "        \"\"\"Load latest model checkpoint\"\"\"\n",
        "        pattern = f\"model_N{N}_{topology}_epoch*.pt\"\n",
        "        checkpoints = list(self.base_dir.glob(pattern))\n",
        "\n",
        "        if not checkpoints:\n",
        "            return None\n",
        "\n",
        "        # Get latest checkpoint\n",
        "        latest = max(checkpoints, key=lambda p: int(p.stem.split('epoch')[1]))\n",
        "        checkpoint = torch.load(latest, map_location='cpu')\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        epoch = checkpoint['epoch']\n",
        "\n",
        "        print(f\"  ✓ Loaded model from epoch {epoch}: {latest}\")\n",
        "        return epoch\n",
        "\n",
        "    def save_training_history(self, history: Dict, N: int, topology: str):\n",
        "        \"\"\"Save training history\"\"\"\n",
        "        path = self.base_dir / f\"history_N{N}_{topology}.json\"\n",
        "        with open(path, 'w') as f:\n",
        "            json.dump(history, f, indent=2)\n",
        "\n",
        "    def load_training_history(self, N: int, topology: str) -> Optional[Dict]:\n",
        "        \"\"\"Load training history\"\"\"\n",
        "        path = self.base_dir / f\"history_N{N}_{topology}.json\"\n",
        "        if path.exists():\n",
        "            with open(path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        return None\n",
        "\n",
        "    def save_benchmark_result(self, result: Dict, N: int, topology: str):\n",
        "        \"\"\"Save benchmark result\"\"\"\n",
        "        path = self.base_dir / f\"benchmark_N{N}_{topology}.json\"\n",
        "        with open(path, 'w') as f:\n",
        "            json.dump(result, f, indent=2)\n",
        "\n",
        "    def load_benchmark_result(self, N: int, topology: str) -> Optional[Dict]:\n",
        "        \"\"\"Load benchmark result\"\"\"\n",
        "        path = self.base_dir / f\"benchmark_N{N}_{topology}.json\"\n",
        "        if path.exists():\n",
        "            with open(path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        return None\n",
        "\n",
        "    def is_complete(self, N: int, topology: str) -> bool:\n",
        "        \"\"\"Check if experiment is complete\"\"\"\n",
        "        path = self.base_dir / f\"benchmark_N{N}_{topology}.json\"\n",
        "        return path.exists()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# OPTIMIZED SPIN SYSTEM (GPU-ready, sparse operations)\n",
        "# ============================================================================\n",
        "\n",
        "class SpinSystemOptimized:\n",
        "    \"\"\"GPU-optimized exact simulator for coupled spin-1/2 systems\"\"\"\n",
        "\n",
        "    def __init__(self, N: int, topology: str = 'chain', device: str = 'cpu'):\n",
        "        self.N = N\n",
        "        self.dim = 2 ** N\n",
        "        self.topology = topology\n",
        "        self.device = device\n",
        "\n",
        "        # For large N, we only support CPU with sparse matrices\n",
        "        if N > 10:\n",
        "            self.device = 'cpu'\n",
        "            self.use_sparse = True\n",
        "        else:\n",
        "            self.use_sparse = False\n",
        "\n",
        "        self._build_operators()\n",
        "\n",
        "    def _kron_list(self, ops: List[np.ndarray], sparse: bool = False):\n",
        "        \"\"\"Kronecker product of list of operators\"\"\"\n",
        "        if sparse:\n",
        "            result = sp.csr_matrix(ops[0])\n",
        "            for op in ops[1:]:\n",
        "                result = sp.kron(result, op)\n",
        "            return result\n",
        "        else:\n",
        "            result = ops[0]\n",
        "            for op in ops[1:]:\n",
        "                result = np.kron(result, op)\n",
        "            return result\n",
        "\n",
        "    def _build_operators(self):\n",
        "        \"\"\"Build spin operators for all sites\"\"\"\n",
        "        # Pauli matrices\n",
        "        sx = np.array([[0, 1], [1, 0]], dtype=complex)\n",
        "        sy = np.array([[0, -1j], [1j, 0]], dtype=complex)\n",
        "        sz = np.array([[1, 0], [0, -1]], dtype=complex)\n",
        "        identity = np.eye(2, dtype=complex)\n",
        "\n",
        "        if self.use_sparse:\n",
        "            sx = sp.csr_matrix(sx)\n",
        "            sy = sp.csr_matrix(sy)\n",
        "            sz = sp.csr_matrix(sz)\n",
        "            identity = sp.eye(2, dtype=complex, format='csr')\n",
        "\n",
        "        self.Ix = []\n",
        "        self.Iy = []\n",
        "        self.Iz = []\n",
        "\n",
        "        for i in range(self.N):\n",
        "            ops = [identity] * self.N\n",
        "\n",
        "            ops[i] = sx\n",
        "            self.Ix.append(self._kron_list(ops, self.use_sparse))\n",
        "\n",
        "            ops[i] = sy\n",
        "            self.Iy.append(self._kron_list(ops, self.use_sparse))\n",
        "\n",
        "            ops[i] = sz\n",
        "            self.Iz.append(self._kron_list(ops, self.use_sparse))\n",
        "\n",
        "    def get_coupling_pairs(self) -> List[Tuple[int, int]]:\n",
        "        \"\"\"Get list of coupled spin pairs based on topology\"\"\"\n",
        "        pairs = []\n",
        "\n",
        "        if self.topology == 'chain':\n",
        "            pairs = [(i, i+1) for i in range(self.N-1)]\n",
        "        elif self.topology == 'ring':\n",
        "            pairs = [(i, (i+1) % self.N) for i in range(self.N)]\n",
        "        elif self.topology == 'star':\n",
        "            pairs = [(0, i) for i in range(1, self.N)]\n",
        "\n",
        "        return pairs\n",
        "\n",
        "    def build_hamiltonian(self, Omega: np.ndarray, J: float):\n",
        "        \"\"\"Build Hamiltonian matrix\"\"\"\n",
        "        if self.use_sparse:\n",
        "            H = sp.csr_matrix((self.dim, self.dim), dtype=complex)\n",
        "        else:\n",
        "            H = np.zeros((self.dim, self.dim), dtype=complex)\n",
        "\n",
        "        # Chemical shift terms\n",
        "        for i in range(self.N):\n",
        "            H = H + Omega[i] * self.Iz[i]\n",
        "\n",
        "        # J-coupling terms\n",
        "        pairs = self.get_coupling_pairs()\n",
        "        for i, j in pairs:\n",
        "            if self.use_sparse:\n",
        "                H = H + 2 * np.pi * J * (\n",
        "                    self.Ix[i].multiply(self.Ix[j]) +\n",
        "                    self.Iy[i].multiply(self.Iy[j]) +\n",
        "                    self.Iz[i].multiply(self.Iz[j])\n",
        "                )\n",
        "            else:\n",
        "                H = H + 2 * np.pi * J * (\n",
        "                    self.Ix[i] @ self.Ix[j] +\n",
        "                    self.Iy[i] @ self.Iy[j] +\n",
        "                    self.Iz[i] @ self.Iz[j]\n",
        "                )\n",
        "\n",
        "        return H\n",
        "\n",
        "    def simulate(self, Omega: np.ndarray, J: float, T: int, dt: float = 1e-4) -> Dict:\n",
        "        \"\"\"Simulate spin dynamics - optimized version\"\"\"\n",
        "        H = self.build_hamiltonian(Omega, J)\n",
        "\n",
        "        # Initial state\n",
        "        psi0 = np.ones(self.dim, dtype=complex) / np.sqrt(self.dim)\n",
        "\n",
        "        times = np.arange(T) * dt\n",
        "        Mx = np.zeros(T)\n",
        "        My = np.zeros(T)\n",
        "        I1z = np.zeros(T)\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Precompute sum operators\n",
        "        if self.use_sparse:\n",
        "            Ix_sum = sum(self.Ix)\n",
        "            Iy_sum = sum(self.Iy)\n",
        "            Iz_first = self.Iz[0]\n",
        "        else:\n",
        "            Ix_sum = sum(self.Ix)\n",
        "            Iy_sum = sum(self.Iy)\n",
        "            Iz_first = self.Iz[0]\n",
        "\n",
        "        # Use more efficient propagation for sparse matrices\n",
        "        if self.use_sparse:\n",
        "            for t_idx, t in enumerate(times):\n",
        "                psi_t = expm_multiply(-1j * H * t, psi0)\n",
        "\n",
        "                Mx[t_idx] = np.real(np.conj(psi_t) @ (Ix_sum @ psi_t))\n",
        "                My[t_idx] = np.real(np.conj(psi_t) @ (Iy_sum @ psi_t))\n",
        "                I1z[t_idx] = np.real(np.conj(psi_t) @ (Iz_first @ psi_t))\n",
        "\n",
        "                if t_idx % 50 == 0:\n",
        "                    print(f\"    Progress: {t_idx}/{T} steps\", end='\\r')\n",
        "        else:\n",
        "            # For small systems, compute once\n",
        "            U = expm(-1j * H * dt)\n",
        "            psi_t = psi0.copy()\n",
        "\n",
        "            for t_idx in range(T):\n",
        "                Mx[t_idx] = np.real(np.conj(psi_t) @ Ix_sum @ psi_t)\n",
        "                My[t_idx] = np.real(np.conj(psi_t) @ Iy_sum @ psi_t)\n",
        "                I1z[t_idx] = np.real(np.conj(psi_t) @ Iz_first @ psi_t)\n",
        "\n",
        "                psi_t = U @ psi_t\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        return {\n",
        "            'Mx': Mx,\n",
        "            'My': My,\n",
        "            'I1z': I1z,\n",
        "            'times': times,\n",
        "            'elapsed_time': elapsed\n",
        "        }\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# DATASET WITH CACHING\n",
        "# ============================================================================\n",
        "\n",
        "class NMRDataset(Dataset):\n",
        "    \"\"\"Dataset of NMR trajectories with caching support\"\"\"\n",
        "\n",
        "    def __init__(self, N: int, topology: str, n_samples: int, T: int, dt: float):\n",
        "        self.N = N\n",
        "        self.topology = topology\n",
        "        self.n_samples = n_samples\n",
        "        self.T = T\n",
        "        self.dt = dt\n",
        "        self.data = []\n",
        "\n",
        "    def generate_data(self):\n",
        "        \"\"\"Generate training data with progress updates\"\"\"\n",
        "        if self.n_samples == 0:\n",
        "            return\n",
        "\n",
        "        print(f\"Generating {self.n_samples} trajectories for N={self.N}, topology={self.topology}...\")\n",
        "\n",
        "        system = SpinSystemOptimized(self.N, self.topology)\n",
        "\n",
        "        for i in range(self.n_samples):\n",
        "            # Random parameters\n",
        "            Omega = np.random.uniform(-100, 100, self.N) * 2 * np.pi\n",
        "            J = np.random.uniform(5, 20)\n",
        "\n",
        "            # Simulate\n",
        "            result = system.simulate(Omega, J, self.T, self.dt)\n",
        "\n",
        "            # Store\n",
        "            params = np.concatenate([Omega, [J]])\n",
        "            observables = np.stack([result['Mx'], result['My'], result['I1z']], axis=1)\n",
        "\n",
        "            self.data.append({\n",
        "                'params': params,\n",
        "                'observables': observables\n",
        "            })\n",
        "\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f\"  Generated {i+1}/{self.n_samples}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        params = torch.tensor(item['params'], dtype=torch.float32)\n",
        "        observables = torch.tensor(item['observables'], dtype=torch.float32)\n",
        "        return params, observables\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# NEURAL OPERATOR (same as before)\n",
        "# ============================================================================\n",
        "\n",
        "class SpectralConv1d(nn.Module):\n",
        "    \"\"\"1D Fourier layer\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, modes: int):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.modes = modes\n",
        "\n",
        "        self.scale = 1 / (in_channels * out_channels)\n",
        "        self.weights = nn.Parameter(\n",
        "            self.scale * torch.rand(in_channels, out_channels, self.modes, 2, dtype=torch.float32)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size = x.shape[0]\n",
        "        x_ft = torch.fft.rfft(x, dim=-1)\n",
        "\n",
        "        out_ft = torch.zeros(batch_size, self.out_channels, x.size(-1)//2 + 1,\n",
        "                            dtype=torch.cfloat, device=x.device)\n",
        "\n",
        "        out_ft[:, :, :self.modes] = torch.einsum(\n",
        "            \"bix,iox->box\",\n",
        "            x_ft[:, :, :self.modes],\n",
        "            torch.view_as_complex(self.weights)\n",
        "        )\n",
        "\n",
        "        x_out = torch.fft.irfft(out_ft, n=x.size(-1), dim=-1)\n",
        "        return x_out\n",
        "\n",
        "\n",
        "class PhysicsInformedFNO(nn.Module):\n",
        "    \"\"\"FNO with physics-informed losses\"\"\"\n",
        "\n",
        "    def __init__(self, modes: int = 16, width: int = 64, n_layers: int = 4,\n",
        "                 n_params: int = 13, n_outputs: int = 3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.modes = modes\n",
        "        self.width = width\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.param_encoder = nn.Sequential(\n",
        "            nn.Linear(n_params, width),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(width, width)\n",
        "        )\n",
        "\n",
        "        self.spectral_layers = nn.ModuleList([\n",
        "            SpectralConv1d(width, width, modes) for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.conv_layers = nn.ModuleList([\n",
        "            nn.Conv1d(width, width, 1) for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.output_projection = nn.Sequential(\n",
        "            nn.Linear(width, width),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(width, n_outputs)\n",
        "        )\n",
        "\n",
        "    def forward(self, params: torch.Tensor, time_steps: int) -> torch.Tensor:\n",
        "        batch_size = params.shape[0]\n",
        "\n",
        "        x = self.param_encoder(params)\n",
        "        x = x.unsqueeze(-1).expand(-1, -1, time_steps)\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "            x1 = self.spectral_layers[i](x)\n",
        "            x2 = self.conv_layers[i](x)\n",
        "            x = x1 + x2\n",
        "            if i < self.n_layers - 1:\n",
        "                x = F.gelu(x)\n",
        "\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.output_projection(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def compute_physics_loss(self, predictions: torch.Tensor) -> torch.Tensor:\n",
        "        Mx = predictions[:, :, 0]\n",
        "        My = predictions[:, :, 1]\n",
        "        I1z = predictions[:, :, 2]\n",
        "\n",
        "        M_mag = torch.sqrt(Mx**2 + My**2)\n",
        "        magnitude_loss = F.relu(M_mag - 1.0).mean()\n",
        "\n",
        "        dt_Mx = Mx[:, 1:] - Mx[:, :-1]\n",
        "        dt_My = My[:, 1:] - My[:, :-1]\n",
        "        smoothness_loss = (dt_Mx**2 + dt_My**2).mean()\n",
        "\n",
        "        dt_I1z = I1z[:, 1:] - I1z[:, :-1]\n",
        "        diffusion_loss = F.relu(dt_I1z).mean()\n",
        "\n",
        "        return magnitude_loss + 0.1 * smoothness_loss + 0.1 * diffusion_loss\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# RESUMABLE TRAINING\n",
        "# ============================================================================\n",
        "\n",
        "def train_surrogate_resumable(model: nn.Module, train_loader: DataLoader,\n",
        "                             val_loader: DataLoader, N: int, topology: str,\n",
        "                             epochs: int = 100, lr: float = 1e-3,\n",
        "                             device: str = 'cpu',\n",
        "                             checkpoint_mgr: CheckpointManager = None) -> Dict:\n",
        "    \"\"\"Train neural surrogate with checkpoint support\"\"\"\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
        "\n",
        "    # Try to resume\n",
        "    start_epoch = 0\n",
        "    history = {'train_loss': [], 'val_loss': [], 'physics_loss': []}\n",
        "\n",
        "    if checkpoint_mgr:\n",
        "        loaded_epoch = checkpoint_mgr.load_model(model, N, topology)\n",
        "        loaded_history = checkpoint_mgr.load_training_history(N, topology)\n",
        "\n",
        "        if loaded_epoch is not None:\n",
        "            start_epoch = loaded_epoch + 1\n",
        "            if loaded_history:\n",
        "                history = loaded_history\n",
        "            print(f\"  ✓ Resuming from epoch {start_epoch}\")\n",
        "\n",
        "    if start_epoch >= epochs:\n",
        "        print(f\"  ✓ Training already complete!\")\n",
        "        return history\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        physics_losses = []\n",
        "\n",
        "        for params, observables in train_loader:\n",
        "            params = params.to(device)\n",
        "            observables = observables.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            predictions = model(params, observables.shape[1])\n",
        "\n",
        "            data_loss = F.mse_loss(predictions, observables)\n",
        "            physics_loss = model.compute_physics_loss(predictions)\n",
        "\n",
        "            loss = data_loss + 0.01 * physics_loss\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_losses.append(data_loss.item())\n",
        "            physics_losses.append(physics_loss.item())\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for params, observables in val_loader:\n",
        "                params = params.to(device)\n",
        "                observables = observables.to(device)\n",
        "\n",
        "                predictions = model(params, observables.shape[1])\n",
        "                loss = F.mse_loss(predictions, observables)\n",
        "                val_losses.append(loss.item())\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        avg_train_loss = np.mean(train_losses)\n",
        "        avg_val_loss = np.mean(val_losses)\n",
        "        avg_physics_loss = np.mean(physics_losses)\n",
        "\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['physics_loss'].append(avg_physics_loss)\n",
        "\n",
        "        # Save checkpoint every 10 epochs\n",
        "        if checkpoint_mgr and (epoch + 1) % 10 == 0:\n",
        "            checkpoint_mgr.save_model(model, N, topology, epoch)\n",
        "            checkpoint_mgr.save_training_history(history, N, topology)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Train: {avg_train_loss:.6f}, Val: {avg_val_loss:.6f}, Physics: {avg_physics_loss:.6f}\")\n",
        "\n",
        "    # Final save\n",
        "    if checkpoint_mgr:\n",
        "        checkpoint_mgr.save_model(model, N, topology, epochs-1)\n",
        "        checkpoint_mgr.save_training_history(history, N, topology)\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# RESUMABLE BENCHMARKING\n",
        "# ============================================================================\n",
        "\n",
        "def benchmark_methods_resumable(N_values: List[int], topology: str = 'chain',\n",
        "                               config: ExperimentConfig = None,\n",
        "                               device: str = 'cpu') -> Dict:\n",
        "    \"\"\"Benchmark different simulation methods with checkpointing\"\"\"\n",
        "\n",
        "    checkpoint_mgr = CheckpointManager()\n",
        "\n",
        "    results = {\n",
        "        'N': [],\n",
        "        'exact_time': [],\n",
        "        'surrogate_time': [],\n",
        "        'surrogate_error': []\n",
        "    }\n",
        "\n",
        "    for N in N_values:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Benchmarking N={N}, topology={topology}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Check if already complete\n",
        "        existing = checkpoint_mgr.load_benchmark_result(N, topology)\n",
        "        if existing:\n",
        "            print(f\"  ✓ Results already exist, skipping...\")\n",
        "            for key in results:\n",
        "                if key in existing:\n",
        "                    results[key].append(existing[key])\n",
        "            continue\n",
        "\n",
        "        # Load or generate datasets\n",
        "        train_dataset = checkpoint_mgr.load_dataset(N, topology, 'train', config.T, config.dt)\n",
        "        if train_dataset is None:\n",
        "            train_dataset = NMRDataset(N, topology, config.n_train_samples, config.T, config.dt)\n",
        "            train_dataset.generate_data()\n",
        "            checkpoint_mgr.save_dataset(train_dataset, N, topology, 'train')\n",
        "\n",
        "        val_dataset = checkpoint_mgr.load_dataset(N, topology, 'val', config.T, config.dt)\n",
        "        if val_dataset is None:\n",
        "            val_dataset = NMRDataset(N, topology, config.n_val_samples, config.T, config.dt)\n",
        "            val_dataset.generate_data()\n",
        "            checkpoint_mgr.save_dataset(val_dataset, N, topology, 'val')\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=config.batch_size)\n",
        "\n",
        "        # Train or load model\n",
        "        model = PhysicsInformedFNO(modes=config.modes, width=config.width,\n",
        "                                   n_layers=config.n_layers,\n",
        "                                   n_params=N+1, n_outputs=3)\n",
        "\n",
        "        print(\"Training surrogate...\")\n",
        "        train_surrogate_resumable(model, train_loader, val_loader, N, topology,\n",
        "                                 epochs=config.epochs, lr=config.lr, device=device,\n",
        "                                 checkpoint_mgr=checkpoint_mgr)\n",
        "\n",
        "        # Benchmark\n",
        "        print(\"\\nRunning benchmark...\")\n",
        "        system = SpinSystemOptimized(N, topology)\n",
        "        Omega = np.random.uniform(-100, 100, N) * 2 * np.pi\n",
        "        J = 12.5\n",
        "\n",
        "        # Exact method\n",
        "        print(\"  Running exact method...\")\n",
        "        exact_result = system.simulate(Omega, J, config.T, config.dt)\n",
        "        exact_time = exact_result['elapsed_time']\n",
        "\n",
        "        # Surrogate method\n",
        "        print(\"  Running surrogate method...\")\n",
        "        model.eval()\n",
        "        model = model.to(device)\n",
        "        params_tensor = torch.tensor(np.concatenate([Omega, [J]]),\n",
        "                                     dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "        start = time.time()\n",
        "        with torch.no_grad():\n",
        "            pred = model(params_tensor, config.T)\n",
        "        surrogate_time = time.time() - start\n",
        "\n",
        "        pred = pred.squeeze().cpu().numpy()\n",
        "\n",
        "        # Compute error\n",
        "        surrogate_error = np.sqrt(\n",
        "            np.mean((exact_result['Mx'] - pred[:, 0])**2) +\n",
        "            np.mean((exact_result['My'] - pred[:, 1])**2) +\n",
        "            np.mean((exact_result['I1z'] - pred[:, 2])**2)\n",
        "        )\n",
        "\n",
        "        # Store results\n",
        "        result = {\n",
        "            'N': N,\n",
        "            'exact_time': exact_time,\n",
        "            'surrogate_time': surrogate_time,\n",
        "            'surrogate_error': float(surrogate_error),\n",
        "            'speedup': exact_time / surrogate_time\n",
        "        }\n",
        "\n",
        "        checkpoint_mgr.save_benchmark_result(result, N, topology)\n",
        "\n",
        "        for key in results:\n",
        "            if key in result:\n",
        "                results[key].append(result[key])\n",
        "\n",
        "        print(f\"\\n✓ Results for N={N}:\")\n",
        "        print(f\"  Exact time: {exact_time:.4f}s\")\n",
        "        print(f\"  Surrogate time: {surrogate_time:.6f}s\")\n",
        "        print(f\"  Speedup: {result['speedup']:.1f}x\")\n",
        "        print(f\"  Error: {surrogate_error:.6f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    # Auto-detect device\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"\\nUsing device: {device}\")\n",
        "\n",
        "    # Configuration\n",
        "    config = ExperimentConfig(\n",
        "        N_values=[4, 6, 8, 10],  # Start smaller\n",
        "        topologies=['chain'],\n",
        "        n_train_samples=50,  # Reduced for faster iteration\n",
        "        n_val_samples=10,\n",
        "        T=100,  # Reduced time steps\n",
        "        dt=1e-4,\n",
        "        epochs=50,  # Reduced epochs\n",
        "        batch_size=8,\n",
        "        lr=1e-3,\n",
        "        modes=16,\n",
        "        width=64,\n",
        "        n_layers=4\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"NMR SPIN DYNAMICS - RESUMABLE BENCHMARK\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Config: N={config.N_values}, T={config.T}, Epochs={config.epochs}\")\n",
        "    print(f\"Device: {device}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Run benchmark\n",
        "    results = benchmark_methods_resumable(\n",
        "        config.N_values,\n",
        "        topology='chain',\n",
        "        config=config,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Save final results\n",
        "    with open('final_results.json', 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"BENCHMARK COMPLETE\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Results saved to: final_results.json\")\n",
        "    print(f\"Checkpoints saved to: checkpoints/\")\n",
        "\n",
        "    # Generate scaling plot if we have results\n",
        "    if len(results['N']) > 0:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.semilogy(results['N'], results['exact_time'], 'o-',\n",
        "                    label='Exact', linewidth=2, markersize=8)\n",
        "        plt.semilogy(results['N'], results['surrogate_time'], 's-',\n",
        "                    label='Surrogate', linewidth=2, markersize=8)\n",
        "        plt.xlabel('Number of Spins (N)', fontsize=14)\n",
        "        plt.ylabel('Time (seconds)', fontsize=14)\n",
        "        plt.title('Computational Scaling (Resumable)', fontsize=16)\n",
        "        plt.legend(fontsize=12)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('scaling_resumable.png', dpi=300, bbox_inches='tight')\n",
        "        print(\"Saved: scaling_resumable.png\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}